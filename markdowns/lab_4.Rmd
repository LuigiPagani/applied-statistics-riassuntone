---
editor_options:
  markdown:
    wrap: 72
  chunk_output_type: console
output:
  html_document:
    df_print: paged
---

\newpage

# LAB 4: Testing for multivariate normality

## Generation of random multivariate Gaussian sample

```{r}
library(mvtnorm)
library(mvnormtest)
library(car) # to draw ellipses
```

We generate a sample of `n=150` observation from bivariate Gaussian (as in `LAB_3.R`)

```{r}
mu  <- c(1,2)
sig <- matrix(c(1,1,1,2), 2)
n   <- 150

set.seed(12345)
X <- rmvnorm(n, mu, sig)

x.1 <- seq(-4,6,0.15)
x.2 <- seq(-4,8,0.15)
w <- matrix(NA, length(x.1), length(x.2)) # create matrix of NAs
for(i in 1:length(x.1)){  
  for(j in 1:length(x.2)){
    w[i,j] <- dmvnorm(c(x.1[i],x.2[j]),mu,sig)
  }
}
image(x.1, x.2, w,asp=1, ylim=c(-4,8), main="Sample points")
points(X[,1],X[,2],pch=20,cex=.75)
```

## Test of normality

### Approach 1: look at some linear combinations of the original variables

Example: original variables, principal components

We first do some visualization

```{r}

par(mfrow=c(2,2))

hist(X[,1], prob=T, ylab='density', xlab='X.1', main='Histogram of X.1',ylim=c(0,0.45))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(X[,1]),sd(X[,1])), col='blue', lty=2)

hist(X[,2], prob=T, ylab='density', xlab='X.2', main='Histogram of X.2',ylim=c(0,0.45))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(X[,2]),sd(X[,2])), col='blue', lty=2)

qqnorm(X[,1], main='QQplot of X.1',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(X[,1])

qqnorm(X[,2], main='QQplot of X.2',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(X[,2])
```

We perform univariate tests of normality on the two components

```{r}
shapiro.test(X[,1])
shapiro.test(X[,2])
```

Recall: **Shapiro-Wilk test**

$H_0: X \sim N$ vs $H_1: H_0^c$

Test statistics:
$$W=\frac{(\text{angular coeff. of the qqline})^2}{\text{sample variance}}$$
One can prove that:

-   $w<1$
-   the empirical distribution under $H_0$ is concentrated on values
    near 1
    -   (if $n=50$ more than 90% of the obs. is between 0.95 and 1);
    -   (if $n=5$ more than 90% of the obs. is between 0.81 and 1).

If the data do NOT come from a Gaussian distribution, the distribution of the test statistics moves towards smaller values: small values of the statistics W give evidence against $H_0$

**Reject** $H_0$ for small values of $W$.

We look at the directions of the PCs

```{r}
pc.X <- princomp(X,scores=T)
head(pc.X$scores)

par(mfrow=c(2,2))

hist(pc.X$scores[,1], prob=T, ylab='density', xlab='comp.1', main='Histogram of PC1',ylim=c(0,0.41))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(pc.X$scores[,1]),sd(pc.X$scores[,1])), col='blue', lty=2)

hist(pc.X$scores[,2], prob=T, ylab='density', xlab='comp.2', main='Histogram of PC2',ylim=c(0,0.7))
lines((-1000):1000 /100, dnorm((-1000):1000 /100,mean(pc.X$scores[,2]),sd(pc.X$scores[,2])), col='blue', lty=2)

qqnorm(pc.X$scores[,1], main='QQplot of PC1',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(pc.X$scores[,1])

qqnorm(pc.X$scores[,2], main='QQplot of PC2',xlab='theoretical quantiles', ylab='sample quantiles')
qqline(pc.X$scores[,2])

shapiro.test(pc.X$scores[,1])
shapiro.test(pc.X$scores[,2])
```

**Problem: which level should I use in each test, to get a global level of** $\alpha$?

### Approach 2: Consider the squared Mahalanobis distances of the data from the (sample) mean and test if they are a sample from a chi-square distribution

Recall this Theorem:
$$X\sim N (\mu,\Sigma)\in R^p,\det(\Sigma)>0 \implies d^2(X,\mu)=(X-\mu)'\Sigma^{-1}(X-\mu)\sim\chi^2(p)$$

```{r}

plot(X, asp=1,xlab='X.1',ylab='X.2')
for(prob in (1:9)/10)
  dataEllipse(X, levels = prob , add=T)

d2 <- mahalanobis(X, colMeans(X), cov(X))

par(mfrow=c(1,2))
hist(d2, prob=T, main='Histogram of the Mahalanobis dist.',
     xlab='d2',ylab='density', col='grey84')
lines(0:2000/100, dchisq(0:2000/100,2), col='blue', lty=2, lwd=2)
qqplot(qchisq((1:n - 0.5)/n, df = 2), d2, main='QQplot of (sample) d2',xlab='theoretical quantiles Chi-sq(2)',
       ylab='sample quantiles')
abline(0, 1)
```

We can perform a **chi.sq goodness-of-fit test**

```{r}
d2.class <- cut(d2, qchisq((0:10)/10, df = 2))
d2.freq  <- table(d2.class)

chisq.test(x = d2.freq, p = rep(1/10, 10), simulate.p.value = T)
```

Test: does the population probabilities (given in x) equal those in p?

Remark: since the mean and covariance matrix are unknown, we can only have approximate solutions. The Mahalanobis distance is computed with estimates of the mean vector and of the covariance matrix; the sample of distances is not iid.

### Approach 3: test of all the directions simultaneously, by looking at the min of the Shapiro-Wilk statistics

We reject $H_0$: X \~ N if we observe a "low" value of `W` along at least one direction, i.e., if the minimum of `W` along the direction is "low".

Looking at all directions, is equivalent to looking at `\min(W)` along the directions (test statistic).

Example with our simulated data: we compute the `W` statistics for all the directions.

```{r}
theta   <- seq(0, pi - pi/180, by = pi/180)
W       <- NULL
P       <- NULL
for(i in 1:length(theta))
{
  a   <- c(cos(theta[i]), sin(theta[i]))
  w   <- shapiro.test(X %*% a)$statistic
  p   <- shapiro.test(X %*% a)$p.value
  W   <- c(W, w)
  P   <- c(P, p)
}
par(mfrow = c(2,1))
plot(theta, W, main = 'W statistics', ylim = c(0.95,1), type='l')
abline(v=c(0, pi/2), col = 'blue')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]), col='red')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]) + pi, col='red')

plot(theta, P, main = 'P-values', ylim = c(0,1), type='l')
abline(v=c(0, pi/2), col = 'blue')
abline(h=0.10, col = 'blue', lty = 2) # set alpha=10%
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]), col='red')
abline(v= atan(princomp(X)$loadings[2,]/princomp(X)$loadings[1,]) + pi, col='red')
```

Note: to set the rejection region, we should look at the distribution of `\min(W)` under $H_0$ [not the distribution of `W.a` for all the directions `a`]

To see how much we reject globally if we set a threshold `alpha=10%` at the univariate tests based on `W.a`, see `Experiment.R`.

Hence, to build the rejection region for the test, we just need to set the threshold as the quantile of order alpha of the distribution of `min(W)` under $H_0$

Formally: $H_0: X \sim N$ vs $H_1: H_0^c$

Test statistics: $\min(W) \sim F$ under $H_0$

Reject $H_0$ if $\min(W)<q_F(\alpha)$, with $q_F(\alpha)$ s.t.
$P(\min(W) < q_F(\alpha)|H_0) = \alpha$.

The distribution $F$ of $\min(W)$ is not known, but it can be approximated with a Monte Carlo method. That is, we approximate the distribution of $\min(W)$ with a histogram generated by simulating Gaussian samples. The quantile $q_F(\alpha)$ is estimated with the sample quantile of order $\alpha$ from the samples.

Note: an explicit expression is available for $\min(W)$. It is computed by the function `mshapiro.test`.

Example: for a 200 sample we can compute $\min(W)$ and look at its distribution.

I'll reject $H_0$ if $\min(W)<q_F(\alpha)$, with $q_F(\alpha)\sim 0.98$

```{r}
min.W=NULL
for(i in 1:200)
{
  Y <- rmvnorm(n, mu, sig)
  min.W <- c(min.W, mshapiro.test(t(Y))$stat)
}
hist(min.W, prob=T, col='grey81', main='Histogram of min(W)'); box()
abline(v=quantile(min.W, probs = .1), col=2)
text(quantile(min.W, probs = .1), 85, labels = expression(q[F](1-alpha)),pos=2)

quantile(min.W, probs = .1)

min.W0 = mshapiro.test(t(X))$stat # Actual observation
min.W0 # accept H0
abline(v=min.W0, col='blue')
```

Approximate the p-value with Monte Carlo: count how many realizations under $H_0$ are associated with a $\min(W)$ lower than the actual observation.

Proportion of the realization that has $\min(W)$ lower than min.W0

```{r}
sum(min.W < min.W0)/200
```

Very high p-value, accept $H_0$.

The function `mcshapiro.test` implements a procedure that:

1. approximates the distribution of the statistics $\min(W)$ via MC

2. performs a test of normality based on the (approximate) distribution of $\min(W)$

3. returns an approximate p-value of the test at point 2.

```{r}
mcshapiro.test <- function(X, devstmax = 0.01, sim = ceiling(1/(4*devstmax^2)))
{
  library(mvnormtest)
  n   <- dim(X)[1]
  p   <- dim(X)[2]
  mu  <- rep(0,p)
  sig <- diag(p)
  W   <- NULL
  for(i in 1:sim)
  {
    Xsim <- rmvnorm(n, mu, sig)
    W   <- c(W, mshapiro.test(t(Xsim))$stat)
    # mshapiro.test(X): compute the statistics min(W) for the sample X
  }
  Wmin   <- mshapiro.test(t(X))$stat   # min(W) for the given sample
  pvalue <- sum(W < Wmin)/sim          # proportion of min(W) more extreme than the observed Wmin
  devst  <- sqrt(pvalue*(1-pvalue)/sim)
  list(Wmin = as.vector(Wmin), pvalue = pvalue, devst = devst, sim = sim)
}

mcshapiro.test(X)
```

## Example: test of normality for the dataset `stiff`.

Dataset: each sample unit is a board. For each board, four measures of stiffness are taken (`X1`: sending a shock wave, `X2`: while vibrating the board, `X3` and `X4`: static tests).
```{r}
stiff <- read.table(here::here('markdowns','lab_4_data','stiff.dat'))
head(stiff)
plot(stiff, asp=1, pch=19)
```

### Normality of the components
```{r}
par(mfcol=c(2,4))

for(i in 1:4)
{
  hist(stiff[,i], prob=T, main=paste('Histogram of V', i, sep=''), xlab=paste('V', i, sep=''))
  lines(900:2800, dnorm(900:2800,mean(stiff[,i]),sd(stiff[,i])), col='blue', lty=2)
  qqnorm(stiff[,i], main=paste('QQplot of V', i, sep=''))
  qqline(stiff[,i])
  print(shapiro.test(stiff[,i])$p)
}
```

### Normality of the principal components
```{r}
PCs <- data.frame(princomp(stiff)$scores)

plot(PCs, asp=1, pch=19)

par(mfcol=c(2,4))
for(i in 1:4)
{
  hist(PCs[,i], prob=T, main=paste('Histogram of PC', i, sep=''))
  lines(seq(min(PCs[,i]), max(PCs[,i]), length=2000), dnorm(seq(min(PCs[,i]), max(PCs[,i]), length=2000),mean(PCs[,i]),sd(PCs[,i])), col='blue', lty=2)
  qqnorm(PCs[,i], main=paste('QQplot of PC', i, sep=''))
  qqline(PCs[,i])
  print(shapiro.test(PCs[,i])$p)
}
```

### Mahalanobis distances of the data from the sample mean
```{r}
M <- colMeans(stiff)
S <- cov(stiff)
d2 <- matrix(mahalanobis(stiff, M, S))

par(mfrow=c(1,2))

hist(d2, prob=T)
lines(0:2000/100, dchisq(0:2000/100,4), col='blue', lty=2)

qqplot(qchisq(seq(0.5/30, 1 - 0.5/30 ,by = 1/30), df = 4), d2,  main='QQplot di d2')
abline(0, 1)

d2.class <- cut(d2, qchisq((0:6)/6, df = 4))
d2.freq  <- table(d2.class)

chisq.test(x = d2.freq, p = rep(1/6, 6), simulate.p.value = T)
```
**Test of all the directions simultaneously.**
```{r}
mcshapiro.test(stiff)
```

**The data don't seem Gaussian.** What can we do?

- Identify clusters 
- Identify (and possibly remove) outliers
- Transform the data (e.g., Box-Cox transformations, see Johnson-Wichern Chap. 4.8, R functions `powerTransform()`; `bcPower()`)
- Work without the Gaussian assumption (e.g., permutation tests)

Let's try to identify and remove outliers.

We remove the data too far (in the sense of the Mahalanobis distance) from the center of the distribution.

```{r}
plot(d2, pch=ifelse(d2<7.5,1,19))
plot(stiff, pch=ifelse(d2<7.5,1,19))

stiff.noout <- stiff[which(d2<7.5),]

mcshapiro.test(stiff.noout)
```
In this case removing the outliers solves the problem of non-gaussianity.