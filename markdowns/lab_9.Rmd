---
output: html_document
editor_options: 
  chunk_output_type: console
---
\newpage

```{r include=FALSE}
path_images = paste(here::here("images"),"/", sep="")
knitr::opts_chunk$set(fig.path = path_images,
                      echo = TRUE,
					  dev = "png",
					  dpi = 300,
					  cache = TRUE)
rm(list = ls())
```

# LAB 9: Hierarchical Clustering and K-means

```{r message=FALSE}
library(mvtnorm)
library(rgl)
library(car)

load(here::here('markdowns','lab_5_data','mcshapiro.test.RData'))
```

## Hierarchical Clustering

### Example 1: iris dataset

Let's forget the labels (we perform a cluster analysis, not a discriminant one).

```{r }
species.name <- iris[, 5]
iris4 <- iris[, 1:4]

pairs(iris4)
```

Visually we can see some separation.

#### Dissimilarity matrix

Compute the **dissimilarity matrix** of the data.

```{r }
help(dist)
```

We try 3 different metrics:

```{r }
iris.e <- dist(iris4, method = "euclidean") # 150x150 'matrix'
iris.m <- dist(iris4, method = "manhattan")
iris.c <- dist(iris4, method = "canberra")

par(mfrow = c(1, 3))
image(1:150, 1:150, as.matrix(iris.e), main = "metrics: Euclidean", asp = 1, xlab = "i", ylab = "j")
image(1:150, 1:150, as.matrix(iris.c), main = "metrics: Canberra", asp = 1, xlab = "i", ylab = "j")
image(1:150, 1:150, as.matrix(iris.m), main = "metrics: Manhattan", asp = 1, xlab = "i", ylab = "j")
```

Our data is ordered, so we clearly see that different groups are more distant.

**In reality, data are never ordered according to (unknown) labels!**

```{r }
misc <- sample(150) # random permutation
iris4 <- iris4[misc, ] # shuffle data

iris.e <- dist(iris4, method = "euclidean")
iris.m <- dist(iris4, method = "manhattan")
iris.c <- dist(iris4, method = "canberra")

par(mfrow = c(1, 3))
image(1:150, 1:150, as.matrix(iris.e), main = "metrics: Euclidean", asp = 1, xlab = "i", ylab = "j")
image(1:150, 1:150, as.matrix(iris.c), main = "metrics: Canberra", asp = 1, xlab = "i", ylab = "j")
image(1:150, 1:150, as.matrix(iris.m), main = "metrics: Manhattan", asp = 1, xlab = "i", ylab = "j")
```

#### Hierarchical clustering

We now aim to perform hierarchical clustering of the dataset `iris`, restricting ourselves to the Euclidean distance.

Command `hclust()`

```{r }
help(hclust)

# method is the linkage, and takes as input the dist matrix (the distance has already been chosen when computing that)
iris.es <- hclust(iris.e, method = "single")
iris.ea <- hclust(iris.e, method = "average")
iris.ec <- hclust(iris.e, method = "complete")
```

if we want more detailed information on euclidean-complete clustering:

```{r }
names(iris.ec) # properties we can access
iris.ec$merge # order of aggregation of statistical units / clusters
iris.ec$height # distance at which we have aggregations
iris.ec$order # ordering that allows to avoid intersections in the dendrogram
```

#### Dendrograms

Plot of the dendrograms

```{r }
par(mfrow = c(1, 3))
plot(iris.es, main = "euclidean-single", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
plot(iris.ec, main = "euclidean-complete", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
plot(iris.ea, main = "euclidean-average", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
```

Plot dendrograms (2 clusters) with `rect.hclust(*, k = 2)`

```{r }
par(mfrow = c(1, 3))
plot(iris.es, main = "euclidean-single", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.es, k = 2)
plot(iris.ec, main = "euclidean-complete", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.ec, k = 2)
plot(iris.ea, main = "euclidean-average", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.ea, k = 2)
```

Plot dendrograms (3 clusters) with `rect.hclust(*, k = 3)`

```{r }
par(mfrow = c(1, 3))
plot(iris.es, main = "euclidean-single", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.es, k = 3)
plot(iris.ec, main = "euclidean-complete", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.ec, k = 3)
plot(iris.ea, main = "euclidean-average", hang = -0.1, xlab = "", labels = F, cex = 0.6, sub = "")
rect.hclust(iris.ea, k = 3)
```

How to cut a dendrogram?
We generate vectors of labels through the command `cutree()`

```{r }
help(cutree)
```

Fix `k=2` clusters:

```{r }
cluster.ec <- cutree(iris.ec, k = 2) # euclidean-complete:
cluster.es <- cutree(iris.es, k = 2) # euclidean-single
cluster.ea <- cutree(iris.ea, k = 2) # euclidean-average

head(cluster.ec)
head(cluster.es)
head(cluster.ea)
```

Interpret the clusters. This is something not doable in real life since we usually don't know the true labels.

```{r }
table(label.true = species.name[misc], label.cluster = cluster.es)
table(label.true = species.name[misc], label.cluster = cluster.ec)
table(label.true = species.name[misc], label.cluster = cluster.ea)
```

Good results for `single` and average `linkage`.

```{r }
plot(iris4, col = ifelse(cluster.es == 1, "red", "blue"), pch = 19)
plot(iris4, col = ifelse(cluster.ec == 1, "red", "blue"), pch = 19)
plot(iris4, col = ifelse(cluster.ea == 1, "red", "blue"), pch = 19)
```

#### Cophenetic coefficients

Let's give a score to the algorithms: did they aggregate coherently with
the dissimilarity matrix or not? Compute the cophenetic matrices

```{r }
coph.es <- cophenetic(iris.es)
coph.ec <- cophenetic(iris.ec)
coph.ea <- cophenetic(iris.ea)
```

Compare with dissimilarity matrix (Euclidean distance)

```{r }
layout(rbind(c(0, 1, 0), c(2, 3, 4)))
image(as.matrix(iris.e), main = "Euclidean", asp = 1)
image(as.matrix(coph.es), main = "Single", asp = 1)
image(as.matrix(coph.ec), main = "Complete", asp = 1)
image(as.matrix(coph.ea), main = "Average", asp = 1)
```

Compute cophenetic coefficients

```{r }
es <- cor(iris.e, coph.es)
ec <- cor(iris.e, coph.ec)
ea <- cor(iris.e, coph.ea)

c("Eucl-Single" = es, "Eucl-Compl." = ec, "Eucl-Ave." = ea)
```

Exercise: repeat the analysis with other metrics.

### Example 2: simulated data

#### Univariate case

```{r }
set.seed(123)
```

`x`: vector of NON clustered data

```{r }
x <- 0:124 / 124 + rnorm(125, sd = 0.01)
```

`y`: vector of clustered data (5 clusters)

```{r }
y <- c(
  rnorm(25, mean = 0,    sd = 0.01),
  rnorm(25, mean = 0.25, sd = 0.01),
  rnorm(25, mean = 0.5,  sd = 0.01),
  rnorm(25, mean = 0.75, sd = 0.01),
  rnorm(25, mean = 1,    sd = 0.01)
)

x <- sample(x)
y <- sample(y)

par(mfrow = c(1, 2))
plot(rep(0, 125), x, main = "data: no clust", xlab = "")
plot(rep(0, 125), y, main = "data: clust", xlab = "")

dx <- dist(x)
dy <- dist(y)
hcx <- hclust(dx, method = "single")
hcy <- hclust(dy, method = "single")

par(mfrow = c(1, 2))
plot(hcx, labels = F, cex = 0.5, hang = -0.1, xlab = "", sub = "x")
plot(hcy, labels = F, cex = 0.5, hang = -0.1, xlab = "", sub = "y")

par(mfrow = c(2, 2))
image(as.matrix(dx), asp = 1, main = "not ordered x")
image(as.matrix(dy), asp = 1, main = "not ordered y")
image(as.matrix(dx)[hcx$order, hcx$order], asp = 1, main = "reordered x")
image(as.matrix(dy)[hcy$order, hcy$order], asp = 1, main = "reordered y")
```

#### Bivariate case (example of chaining effect)

Two clusters with a bivariate generated dataset

```{r }
par(mfrow=c(1,1))
p <- 2
n <- 100

mu1 <- c(0, 1)
mu2 <- c(5, 1.2)
sig <- diag(rep(1, p))

set.seed(1)
X1 <- rmvnorm(n, mu1, sig)
X2 <- rmvnorm(n, mu2, sig)

X <- rbind(X1, X2)
```

If we knew the labels:

```{r }
plot(X, xlab = "Var 1", ylab = "Var 2",
     col = rep(c("red", "blue"), each = 100), asp = 1)
```

How we actually see the data:

```{r }
plot(X, xlab = "Var 1", ylab = "Var 2", asp = 1)
```

Let's compare the clustering results with Euclidean distance and three types of linkage

```{r }
x.d <- dist(X, method = "euclidean")

x.es <- hclust(x.d, method = "single")
x.ea <- hclust(x.d, method = "average")
x.ec <- hclust(x.d, method = "complete")

par(mfrow = c(1, 3))
plot(x.es, main = "Single linkage", hang = -0.1, xlab = "", labels = F, sub = "")
plot(x.ea, main = "Average linkage", hang = -0.1, xlab = "", labels = F, sub = "")
plot(x.ec, main = "Complete linkage", hang = -0.1, xlab = "", labels = F, sub = "")
```

Let's cut the tree as to get 2 clusters

```{r }
cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)

par(mfrow = c(1, 3))
plot(X, xlab = "Var 1", ylab = "Var 2", main = "Single linkage",
     col = ifelse(cluster.es == 1, "red", "blue"), pch = 16, asp = 1)
plot(X, xlab = "Var 1", ylab = "Var 2", main = "Average linkage",
     col = ifelse(cluster.ea == 1, "red", "blue"), pch = 16, asp = 1)
plot(X, xlab = "Var 1", ylab = "Var 2", main = "Complete linkage",
     col = ifelse(cluster.ec == 1, "red", "blue"), pch = 16, asp = 1)
```

#### Bivariate case (example of ellipsoidal clusters)

```{r }
p <- 2
n <- 100

mu1 <- c(0, 1)
mu2 <- c(6.5, 1)

e1 <- c(1, 1)
e2 <- c(-1, 1)
sig <- 5 * cbind(e1) %*% rbind(e1) + .1 * cbind(e2) %*% rbind(e2)

set.seed(2)
X1 <- rmvnorm(n, mu1, sig)
X2 <- rmvnorm(n, mu2, sig)

X <- rbind(X1, X2)
```

If we knew the labels:

```{r }
par(mfrow=c(1,1))
plot(X, xlab = "Var 1", ylab = "Var 2", asp = 1, col = rep(c("red", "blue"), each = 100))
```

How we actually see the data:

```{r }
plot(X, xlab = "Var 1", ylab = "Var 2", asp = 1)
```

Compare results with different linkage, when Euclidean distances is used

```{r }
x.d <- dist(X, method = "euclidean")

x.es <- hclust(x.d, method = "single")
x.ea <- hclust(x.d, method = "average")
x.ec <- hclust(x.d, method = "complete")

par(mfrow = c(1, 3))

plot(x.es, main = "Single linkage", ylab = "Euclidean distance", hang = -0.1, xlab = "", labels = F, sub = "")
plot(x.ea, main = "Average linkage", hang = -0.1, xlab = "", labels = F, sub = "")
plot(x.ec, main = "Complete linkage", hang = -0.1, xlab = "", labels = F, sub = "")
```

Let's cut the tree to get 2 clusters

```{r }
par(mfrow = c(1, 3))

plot(x.es, main = "Single linkage", ylab = "Euclidean distance", hang = -0.1, xlab = "", labels = F, sub = "")
rect.hclust(x.es, k = 2)
plot(x.ea, main = "Average linkage", hang = -0.1, xlab = "", labels = F, sub = "")
rect.hclust(x.ea, k = 2)
plot(x.ec, main = "Complete linkage", hang = -0.1, xlab = "", labels = F, sub = "")
rect.hclust(x.ec, k = 2)

cluster.es <- cutree(x.es, k = 2)
cluster.ea <- cutree(x.ea, k = 2)
cluster.ec <- cutree(x.ec, k = 2)

par(mfrow = c(1, 3))

plot(X, xlab = "Var 1", ylab = "Var 2", main = "Euclidean, Single linkage", col = cluster.es + 1, pch = 16, asp = 1)
plot(X, xlab = "Var 1", ylab = "Var 2", main = "Euclidean, Average linkage", col = cluster.ea + 1, pch = 16, asp = 1)
plot(X, xlab = "Var 1", ylab = "Var 2", main = "Euclidean, Complete linkage", col = cluster.ec + 1, pch = 16, asp = 1)
```

Change the mean `mu2`
Example: `mu2 <- c(4.5,1.2)`

### Example 3: `earthquakes` dataset

```{r }
help(quakes)

head(quakes)
dim(quakes)

Q <- cbind(quakes[, 1:2], depth = -quakes[, 3] / 100)
head(Q)

plot3d(Q, size = 3, col = "orange", aspect = F)
```

Dissimilarity matrix (Euclidean metric)

```{r }
d <- dist(Q)

par(mfrow=c(1,1))
image(as.matrix(d))
```

Hierarchical clustering

```{r }
par(mfrow=c(2,2))

clusts <- hclust(d, method = "single")
plot(clusts, hang = -0.1, labels = FALSE, main = "single", xlab = "", sub = "")
rect.hclust(clusts, k=2)
rect.hclust(clusts, k=3)

clusta <- hclust(d, method = "average")
plot(clusta, hang = -0.1, labels = FALSE, main = "average", xlab = "", sub = "")
rect.hclust(clusta, k=2)
rect.hclust(clusta, k=3)

clustc <- hclust(d, method = "complete")
plot(clustc, hang = -0.1, labels = FALSE, main = "complete", xlab = "", sub = "")
rect.hclust(clustc, k=2)
rect.hclust(clustc, k=3)

clustw <- hclust(d, method = "ward.D2")
plot(clustw, hang = -0.1, labels = FALSE, main = "ward", xlab = "", sub = "")
rect.hclust(clustw, k=2)
rect.hclust(clustw, k=3)
```

#### Ward-Linkage (see J-W p. 692-693)

Ward considered hierarchical clustering procedures based on **minimizing the 'loss of information' from joining two groups**. This method is usually implemented with loss of information taken to be an increase in an error sum of squares criterion, **ESS**. First for a given cluster `k`, let `ESS[k]` be the sum of the squared deviations of every item in the cluster from the cluster mean (**centroid**). 

```
(ESS[k]=sum_{x.j in cluster k} t(x.j-x.mean[k])%*%(x.j-x.mean[k])
```

where `x.mean[k]` is the centroid of cluster `k`). If there are currently `K` clusters, define `ESS` as the sum of the `ESS[k]` (`ESS=ESS[1]+ESS[2]+...+ESS[K]`). At each step in the analysis, the union of every possible pair of clusters is considered, and the two clusters whose combination results in the *smallest increase* in `ESS` (minimum loss of information) are joined.

Initially, each cluster consists of a single item and, if there are `N` items, `ESS[k]=0`, `k=1,2,...,N`, so `ESS=0`. At the other extreme, when all the clusters are combined in a single group of `N` items, the value of `ESS` is given by `ESS=sum_j(t(x.j-x.mean)%*%(x.j-x.mean))`, where `x.j` is the multivariate measurement associated with the `j`-th item and `x.mean` is the mean of all items.

#### Single linkage

```{r }
clusters <- cutree(clusts, 2)
plot3d(Q, size = 3, col = clusters + 1, aspect = F)

clusters <- cutree(clusts, 3)
plot3d(Q, size = 3, col = clusters + 1, aspect = F)
```

#### Average linkage

```{r }
clustera <- cutree(clusta, 2)
plot3d(Q, size = 3, col = clustera + 1, aspect = F)

clustera <- cutree(clusta, 3)
plot3d(Q, size = 3, col = clustera + 1, aspect = F)
```

#### Complete linkage

```{r }
clusterc <- cutree(clustc, 2)
plot3d(Q, size = 3, col = clusterc + 1, aspect = F)

clusterc <- cutree(clustc, 3)
plot3d(Q, size = 3, col = clusterc + 1, aspect = F)
```

#### Ward linkage

```{r }
clusterw <- cutree(clustw, 2)
plot3d(Q, size = 3, col = clusterw + 1, aspect = F)

clusterw <- cutree(clustw, 3)
plot3d(Q, size = 3, col = clusterw + 1, aspect = F)
```

## K-means method

Simulated data

```{r }
n <- 100
set.seed(1)
x <- matrix(rnorm(n * 2), ncol = 2)
x[1:(n / 2), 1] <- x[1:(n / 2), 1] + 2 # add 2 to first half of 1st col
x[1:(n / 2), 2] <- x[1:(n / 2), 2] - 2 # sub 2 to first half of 2nd col

par(mfrow=c(1,1))
plot(x, pch = 20, cex = 2, xlab = "x1", ylab = "x2")
```

K-means algorithm

```{r }
k <- 2
cluster <- sample(1:2, n, replace = TRUE)
iter.max <- 3

colplot <- c("royalblue", "red")
colpoints <- c("blue4", "red4")

par(mfrow = c(iter.max, 3))

for (i in 1:iter.max) {
    C <- NULL
    for (l in 1:k) {
        C <- rbind(C, colMeans(x[cluster == l, ]))
    }

    plot(x, col = colplot[cluster], pch = 19)
    line <- readline()

    points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
    line <- readline()

    plot(x, col = "grey", pch = 19)
    points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
    line <- readline()

    QC <- rbind(C, x)
    Dist <- as.matrix(dist(QC, method = "euclidean"))[(k + 1):(k + n), 1:k]
    for (j in 1:n) {
        cluster[j] <- which.min(Dist[j, ])
    }

    plot(x, col = colplot[cluster], pch = 19)
    points(C, col = colpoints, pch = 4, cex = 2, lwd = 2)
    # line <- readline() # when this is on, you have to press space to go to the next iteration
}
```

### K-means for the `earthquakes` dataset

In automatic, command `kmeans()`

```{r }
help(kmeans)

result.k <- kmeans(Q, centers = 2) # Centers: fixed number of clusters

names(result.k)

result.k$cluster # labels of clusters
result.k$centers # centers of the clusters
result.k$totss # tot. sum of squares
result.k$withinss # sum of squares within clusters
result.k$tot.withinss # sum(sum of squares within cluster)
result.k$betweenss # sum of squares between clusters
result.k$size # dimension of the clusters

plot(Q, col = result.k$cluster + 1)

plot3d(Q, size = 3, col = result.k$cluster + 1, aspect = F)
points3d(result.k$centers, size = 10)
```

**How to choose `k`:**

1. evaluate the variability between the groups with respect to the variability withing the groups
2. evaluate the result of hierarchical clustering (not recommended, quite computationally expensive)

We've just seen method 2. and suggested `k=2`!

Method 1.

```{r }
b <- NULL
w <- NULL
for (k in 1:10) {
    result.k <- kmeans(Q, centers = k)
    w <- c(w, sum(result.k$withinss))
    b <- c(b, result.k$betweenss)
}

par(mfrow=c(1,1))
matplot(1:10, w / (w + b), pch = "", xlab = "# clusters", ylab = "within/tot", main = "Choice of k", ylim = c(0, 1)) # empty but right size
lines(1:10, w / (w + b), type = "b", lwd = 2) # adding the line
```

This method seems to suggest `k=2` or `3`. Let's try also `k=3`:

```{r }
result.k <- kmeans(Q, 3)

plot(Q, col = result.k$cluster + 1)

plot3d(Q, size = 3, col = result.k$cluster + 1, aspect = F)
points3d(result.k$centers, size = 10)
```

## Problem 3 of July 1, 2009

The Veritatis Diagram is a mysterious work attributed to Galileo. Some semiologists believe that some pages of the book hide a coded message; they also believe that these pages are characterized by an abnormal numerosity of some letters of the alphabet. The `veritatis.txt` file lists, for 132 pages of the book, the absolute frequencies of the five vowels of the Latin alphabet.

a. By using an agglomerative clustering algorithm (Manhattan distance, average linkage), identify two clusters and report suspicious pages;
b. assuming that, for each of the two clusters, the five absolute frequencies are (approximately) normally distributed with the same covariance matrix perform a test to prove the existence of a difference in the mean value of the two distributions;
c. using five Bonferroni intervals of global confidence 90%, comment the results of the test at point b.

```{r }
vowels <- read.table(here::here("markdowns","lab_9_data","veritatis.txt"), header = T)
head(vowels)
dim(vowels)
```

Question a.

```{r }
plot(vowels)

HC <- hclust(dist(vowels, method = "manhattan"), method = "average")

plot(HC, hang = -0.1, sub = "", labels = F, xlab = "")

# we cut the dendrogram at k=2 clusters
rect.hclust(HC, k = 2)

pag <- cutree(HC, k = 2)
table(pag)
which(pag == 2)

plot(vowels, col = pag + 1, asp = 1, pch = 16, lwd = 2)
```

Question b.

```{r }
p <- 5
n1 <- table(pag)[1]
n2 <- table(pag)[2]
```

Verify Gaussianity

```{r }
mcshapiro.test(vowels[pag == "1", ])
mcshapiro.test(vowels[pag == "2", ])
```

Test for independent Gaussian populations

```{r }
t1.mean <- sapply(vowels[pag == "1", ], mean)
t2.mean <- sapply(vowels[pag == "2", ], mean)
t1.cov <- cov(vowels[pag == "1", ])
t2.cov <- cov(vowels[pag == "2", ])
Sp <- ((n1 - 1) * t1.cov + (n2 - 1) * t2.cov) / (n1 + n2 - 2)
```

Test: $H_0$: `mu.1-mu.2==0` vs $H_1$: `mu.1-mu.2!=0`

```{r }
delta.0 <- c(0, 0, 0, 0, 0)
Spinv <- solve(Sp)
T2 <- n1 * n2 / (n1 + n2) * (t1.mean - t2.mean - delta.0) %*% Spinv %*% (t1.mean - t2.mean - delta.0)
P <- 1 - pf(T2 / (p * (n1 + n2 - 2) / (n1 + n2 - 1 - p)), p, n1 + n2 - 1 - p)
P
```

We reject the test, i.e. the means are different with strong evidence.

Question c.

```{r }
alpha <- 0.1
IC <- cbind(
  t2.mean - t1.mean - sqrt(diag(Sp) * (1/n1+1/n2)) * qt(1-alpha/(p*2), n1+n2-2),
  t2.mean - t1.mean,
  t2.mean - t1.mean + sqrt(diag(Sp) * (1/n1+1/n2)) * qt(1-alpha/(p*2), n1+n2-2)
)
IC
```

## Problem 2 of February 18, 2009

Friday, October 17, 2008, in Black Fortune skies a crash occurred between two artificial satellites. About a hundred debris were found on the ground (`satellite.txt` file). However, due to friction with the atmosphere it was not possible to define the origin of any debris. To clarify the accident, the US Air Force requests to estimate the relative position of the two points of impact on the ground. Assuming that the debris coming from the same satellite are scattered on the ground according to a normal law of unknown mean and covariance matrix:

a. attribute the debris to the two satellites via a hierarchical clustering algorithm that uses the Euclidean distance and the average linkage;
b. report the numerosity of the two clusters and the value of the cophenetic coefficient;
c. introducing the appropriate hypotheses, identifying the points of impact on the ground with the mean of the two normal distributions and assuming correct the allocation of the debris to the two satellites, provide an elliptical confidence region (global level 99%) for the relative position of the two points of impact on the ground.

```{r }
satellite <- read.table(here::here("markdowns","lab_9_data","satellite.txt"), header = T)
head(satellite)

plot(satellite, asp = 1, pch = 16)
```

Question a.

```{r }
D.s <- dist(satellite)

HCa <- hclust(D.s, method = "average")

plot(HCa, hang = -0.1, sub = "", xlab = "", labels = F)

# we know that there are 2 clusters, so we cut the dendrograms accordingly:
rect.hclust(HCa, k = 2)

sata <- cutree(HCa, k = 2)

plot(satellite, col = sata + 1, asp = 1, pch = 16, main = "Average")
```

Question b.

```{r }
table(sata)

coph.a <- cophenetic(HCa)

coph.sat <- cor(D.s, coph.a)
coph.sat
```

Question c.

```{r }
p <- 2
n1 <- table(sata)[1]
n2 <- table(sata)[2]
```

Assumptions:

- Normality
- Independent populations (ok)
- Homogeneity of covariance structures

Normality

```{r }
mcshapiro.test(satellite[sata == "1", ])$pvalue
mcshapiro.test(satellite[sata == "2", ])$pvalue

t1.mean <- sapply(satellite[sata == "1", ], mean)
t2.mean <- sapply(satellite[sata == "2", ], mean)
t1.cov <- cov(satellite[sata == "1", ])
t2.cov <- cov(satellite[sata == "2", ])
```

Homogeneity of covariance structures

```{r }
t1.cov
t2.cov

Sp <- ((n1-1) * t1.cov + (n2-1) * t2.cov) / (n1+n2-2)
```

Elliptic confidence region at 99%

```{r }
alpha <- 0.01
cfr.fisher <- (p * (n1+n2-2) / (n1+n2-1-p)) * qf(1-alpha, p, n1+n2-1-p)
```

Directions of the axes

```{r }
eigen(Sp)$vector
```

Radius

```{r }
r <- sqrt(cfr.fisher)
```

Length of the semi-axes

```{r }
r * sqrt(eigen(Sp)$values * (1 / n1 + 1 / n2))

par(mfrow = c(1, 2))
plot(satellite, col = sata + 1, asp = 1, pch = 16, main = "Original data and groups")
plot(satellite,
  xlim = c(-23, -15), ylim = c(-25, -17), pch = "", asp = 1,
  main = "Elliptic region for\nthe mean diff. (red - green)", cex.main = 0.8
)

# confidence region and sample mean in blue
ellipse(
  center = t1.mean - t2.mean,
  shape  = Sp * (1/n1 + 1/n2),
  radius = r,
  lwd = 2, col = "blue"
)
```

## Problem 2 of February 28, 2013

In view of the week of *haute couture* in Paris, the fashion house La Boutin has decided to create a unique-edition pair of shoes decorated with gemstones instead of crystals. During the processing of the pair of shoes, a bumbling craftsman caused the fall of the gemstones and of a box of crystals. The file `preziosi.txt` collects data related to the Cartesian coordinates of the found stones.

a. Through a hierarchical clustering algorithm (Euclidean distance and Ward linkage), identify the 2 clusters of stones.
b. reporting the numerosity of the two groups identified at point a) and the value of the cophenetic coefficient.
c. Identify the gemstones with the smaller group. Having introduced (and verified) the appropriate assumptions, estimate an elliptical region containing 99% of gemstones (report the center, the length and the direction of the principal axes of the ellipse).

```{r }
stones <- read.table(here::here("markdowns","lab_9_data","preziosi.txt"), header = TRUE)
par(mfrow=c(1,1))
plot(stones)
```

Question a.

```{r }
p.e <- dist(stones, method = "euclidean")
p.ew <- hclust(p.e, method = "ward.D2")

plot(p.ew, hang = -0.1, sub = "", xlab = "", labels = F)
cl.ew <- cutree(p.ew, k = 2) # euclidean-ward
```

Question b.

```{r }
table(cl.ew)

coph.ew <- cophenetic(p.ew)
ew <- cor(p.e, coph.ew)
ew
```

Question c.

```{r }
p.pr <- stones[which(cl.ew == 2), ] # from the table, 2 is the smallest
n <- dim(p.pr)[1]
p <- dim(p.pr)[2]

plot(stones)
points(p.pr, pch = 19)
```

Normality

```{r }
mcshapiro.test(p.pr)$pvalue

M <- sapply(p.pr, mean)
S <- cov(p.pr)
alpha <- 0.01

cfr.fisher <- (n-1)*p/(n-p)*qf(1-alpha,p,n-p)
# when n large this should approximate the correct one
# cfr.chisq <- qchisq(1 - alpha, p)
```

Characterize the ellipse:
Axes directions:

```{r }
eigen(S)$vectors
```

Center:

```{r }
M
```

Radius of the ellipse:

```{r }
r <- sqrt(cfr.fisher)
```

Length of the semi-axes:

```{r }
r * sqrt(eigen(S)$values)

plot(p.pr, asp = 1, col = "gold", pch = 19, xlim = c(-10, 50))
points(M[1], M[2], pch = 4, cex = 1.5, lwd = 2)

ellipse(center = M, shape = S, radius = r, col = "black", lty = 2, center.pch = 4)
points(stones[which(cl.ew == 1), ])
```

## Problem 3 of February 28, 2007

The dataset `Pb3.txt` reports Length (cm) Width (cm) of the chest of 50 sparrows (in Italian: passeri). The biologist who has collected the measurements aims to demonstrate that the sparrows can be divided into two distinct groups in terms of length and width of the chest. Help him to prove his theory by implementing and commenting on the following analyses:

a. By means of an agglomerative hierarchical clustering algorithm that uses the Manhattan distance and the Single linkage state if it is reasonable to cluster the data into two groups.
b. Implement a test to prove the difference of the means of the two groups.
c. Identify and comment the four Bonferroni intervals with global confidence 90% (lower bound, central value, upper bound) for: 

- The difference of the mean of the variable length. 
- The difference of the mean of the variable width. 
- The difference of the mean of the sum of the variables length and width. 
- The difference of the mean of the difference of variable length and width.

d. verify the assumptions necessary to the implementation of the test.

```{r }
sparrows <- read.table(here::here("markdowns","lab_9_data","Pb3.txt"))
head(sparrows)
dim(sparrows)
```

Question a.

```{r }
plot(sparrows, pch = 16)

groups <- hclust(dist(sparrows, method = "manhattan"), method = "single")
plot(groups, hang = -0.1, sub = "", xlab = "", labels = F)
```

Cut in 2 groups

```{r }
cluster <- cutree(groups, k = 2)

plot(sparrows, pch = 16, col = as.vector(cluster) + 1)
```

Question b.

```{r }
g1 <- sparrows[cluster == 1, ]
g2 <- sparrows[cluster == 2, ]
head(g1)
head(g2)
```

Test: $H_0$: `mu.1-mu.2==0` vs $H_1$: `mu.1-mu.2!=0`

```{r }
p <- 2
n1 <- dim(g1)[1]
n2 <- dim(g2)[1]
alpha <- 0.10

mean1 <- sapply(g1, mean)
mean2 <- sapply(g2, mean)
cov1 <- cov(g1)
cov2 <- cov(g2)
Sp <- ((n1 - 1) * cov1 + (n2 - 1) * cov2) / (n1 + n2 - 2)

delta.0 <- c(0, 0)
Spinv <- solve(Sp)

T2 <- n1 * n2 / (n1 + n2) * (mean1 - mean2 - delta.0) %*% Spinv %*% (mean1 - mean2 - delta.0)

cfr.fisher <- (p * (n1 + n2 - 2) / (n1 + n2 - 1 - p)) * qf(1 - alpha, p, n1 + n2 - 1 - p)

pvalue <- 1 - pf(T2 / (p * (n1 + n2 - 2) / (n1 + n2 - 1 - p)), p, n1 + n2 - 1 - p)
pvalue
```

We have strong evidence that the mean of the two groups is different.

Question c.

```{r }
A <- rbind(c(1,  0), # first variable
           c(0,  1), # second variable
           c(1,  1), # their sum
           c(1, -1)) # their difference
k <- dim(A)[1]

A.s2 <- diag(A %*% Sp %*% t(A))
A.dm <- A %*% (mean1 - mean2)

Bonf <- cbind(
  inf = A.dm - qt(1-(alpha/(2*k)), n1+n2-2) * sqrt(A.s2 * (1/n1 + 1/n2)),
  center = A.dm,
  sup = A.dm + qt(1-(alpha/(2*k)), n1+n2-2) * sqrt(A.s2 * (1/n1 + 1/n2))
)
Bonf
```

Question d.

```{r }
mcshapiro.test(g1)$pvalue
mcshapiro.test(g2)$pvalue

cov1
cov2
```

## Multidimensional Scaling (principal coordinate analysis)

Given the distances (dissimilarities) among $n$ statistical units, look for the $k$-dimensional representation ($k$ small, 2,3) of the $n$ statistical units such that the distances (dissimilarities) among the representations of the $n$ units are as close as possible to the original distances (dissimilarities) among the n units.

### Example 1: European cities

```{r }
help(eurodist)
dim(eurodist) # road distances among European cities
n <- dim(eurodist)[1]
```

R function for multidimensional scaling: `cmdscale`

```{r }
help(cmdscale)

location <- cmdscale(eurodist, k = 2)
location
```

I have to set `asp=1` (equal scales on the two axes) to correctly represent Euclidean distances

```{r }
plot(location[, 1], -location[, 2],
     pch = 16, asp = 1, axes = FALSE, cex = 0.4,
     main = "MDS of European cities", xlab = "", ylab = "")
text(location[, 1], -location[, 2],
     labels = colnames(as.matrix(eurodist)), cex = 0.75, pos = 2)
```

The minus is to get the North in the upper part of the plot.

Compare the original matrix `d_ij = d(x_i,x_j)` and `delta_ij = d(y_i,y_j)`

```{r }
plot(eurodist, dist(location))
```

Visualize the most different distances

```{r }
par(cex = 0.75, mar = c(10, 10, 2, 2))
image(1:21, 1:21, asp = 1,
      abs(as.matrix(dist(location)) - as.matrix(eurodist)),
      axes = F, xlab = "", ylab = "")
axis(1, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 2, cex = 0.75)
axis(2, at = 1:21, labels = colnames(as.matrix(eurodist)), las = 1, cex = 0.75)
box()
par(cex = 1, mar = c(5.1, 4.1, 4.1, 2.1))
```

Rome-Athens

```{r }
as.matrix(eurodist)[19, 1]
as.matrix(dist(location))[19, 1]
```

Cologne-Geneve

```{r }
as.matrix(eurodist)[6, 8]
as.matrix(dist(location))[6, 8]
```

Compute the **stress**: the higher it is, the worse the matching between original distances and their geometrical representation through MDS

```{r }
Stressk <- NULL
for (k in 1:4)
{
  location.k <- cmdscale(eurodist, k)
  Stress <- (sum((as.vector(eurodist) - as.vector(dist(location.k)))^2) /
    sum(as.vector(location.k)^2))^(1 / 2)
  Stressk <- c(Stressk, Stress)
}

plot(1:4, Stressk, xlab = "k", ylab = "Stress", lwd = 2)
```

The stress increases for `k>2` because of numerical problems (the representation of minimal stress is not always found)
